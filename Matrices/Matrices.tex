\documentclass{beamer}
\usepackage{graphicx}
\graphicspath{{Figures/}}
% There are many different themes available for Beamer. A comprehensive
% list with examples is given here:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
% You can uncomment the themes below if you would like to use a different
% one:
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{boxes}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
\usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}


\title[Matrices]{Mathematics/Statistics Bootcamp} % The short title appears at the bottom of every slide, the full title is only on the title page

\subtitle{Part II: Matrix Algebra}

\author{Brian Cozzi\inst{1} \and Michael Valancius\inst{1} \and Graham Tierney\inst{1} \and Becky Tang\inst{1}}


\institute[Duke University] % (optional, but mostly needed)
{
  \inst{1}%
  Department of Statistical Science\\
  Duke University
  }
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{Graduate Orientation, August 2019}


% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}


% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%% Part 1: Probability theory %%%%%
\section{Matrices: The Basics}

\subsection{Matrix Notation and Operations}
\begin{frame}{Notation}
A real \textbf{matrix} (hereafter referred to as a matrix) is a rectangular array of real numbers.  The collection of real numbers within a matrix $a_{11}, a_{12}, \hdots, a_{1n}, \hdots, a_{m1}, a_{m2}, \hdots, a_{mn}$ can be arranged as

\[
\begin{pmatrix}
    a_{11} & a_{12} & a_{13} & \dots  & a_{1n} \\
    a_{21} & a_{22} & a_{23} & \dots  & a_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & a_{m3} & \dots  & a_{mn}
\end{pmatrix}
\]

\textit{How to denote column vectors and row vectors, dimensions of the matrix}

This matrix, which has $m$ rows and $n$ columns, is an $m \times n$ matrix, where $m$ and $n$ are the \textbf{dimensions} of the matrix.  The scalar $a_{ij}$ at the intersection of the $i$th row and $j$th column of a matrix is called the $ij$th entry or element.  Boldface capital letters (e.g. $\mathbf{A}$) are used to represent matrices. \\
\end{frame}


\begin{frame}
\frametitle{Matrix Addition}
The \textbf{sum} of two $m \times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ is denoted by $\mathbf{A} + \mathbf{B}$ and defined to be the $m \times n$ matrix whose $ij$th element is $a_{ij} + b_{ij}$.  For example,

\[
\begin{pmatrix}
    1 & 2\\
    3 & 4 \\
\end{pmatrix}
+
\begin{pmatrix}
    5 & 6\\
    7 & 8 \\
\end{pmatrix}
= 
\begin{pmatrix}
    6 & 8\\
    10 & 12 \\
\end{pmatrix}
\]

Addition is commutative and associative, so 
\[\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}\]

\[\mathbf{A} + (\mathbf{B} + \mathbf{C})= (\mathbf{A} + \mathbf{B}) + \mathbf{C}\]
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication}
Let $\mathbf{A}$ be a $m \times n$ matrix and let $\mathbf{B}$ be a $p \times q$ matrix.  If $n = p$, the matrix \textbf{product} $\mathbf{AB}$ is defined to be the $m \times q$ matrix whose $ij$th element is

\[\mathbf{AB}_{ij} = \sum_{k=1}^n a_{ik}b_{kj}\]

For example, 
\[
\begin{pmatrix}
    1 & 2\\
    3 & 4 \\
\end{pmatrix}
\begin{pmatrix}
    5 & 6\\
    7 & 8 \\
\end{pmatrix}
= 
\begin{pmatrix}
    1(5) + 2(7) & 1(6) + 2(8)\\
    3(5) + 4(7) & 3(6) + 4(8) \\
\end{pmatrix}
=
\begin{pmatrix}
    19 & 22\\
    43 & 50 \\
\end{pmatrix}
\]
\end{frame}

 \begin{frame}
\frametitle{Intuition: Matrix Multiplication}

\href{https://www.youtube.com/watch?v=XkY2DOUCWMU&index=4&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}{\textbf{Matrix multiplication as composition}, by \textit{3Blue1Brown}}

\textit{Maybe skip this}

\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication Properties}
Matrix multiplication is associative, but not commutative in general ($\mathbf{BA}$ may not even be defined!).  That is, if a matrix $\mathbf{C}$ has $q$ rows, then

\[\mathbf{A}(\mathbf{BC}) = (\mathbf{AB})\mathbf{C}\]

In addition, matrix multiplication is distributive with respect to addition, so assuming that matrix dimensions are such that all products and sums are defined,
\begin{align*}
\mathbf{A} (\mathbf{B} + \mathbf{C}) &= \mathbf{AB} + \mathbf{AC}\\
(\mathbf{A} + \mathbf{B}) \mathbf{C}  &= \mathbf{AC} + \mathbf{BC}
\end{align*}

For any scalar $c$, we have that
\begin{align*}
c(\mathbf{A} + \mathbf{B}) &= c\mathbf{A} + c\mathbf{B}\\
c\mathbf{AB} &= (c\mathbf{A})\mathbf{B} = \mathbf{A}(c\mathbf{B})
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Transposition}
The transpose of a $m \times n$ matrix $\mathbf{A}$, denoted by either the symbol $\mathbf{A}^{'}$ or $\mathbf{A}^T$, is the $n \times m$ matrix whose $ij$th element is the $ji$th element of $\mathbf{A}$.  For example,

\[
\begin{pmatrix}
    1 & 2\\
    3 & 4 \\
\end{pmatrix}^T
= 
\begin{pmatrix}
    1 & 3\\
    2 & 4 \\
\end{pmatrix}
\]

Below are some properties of matrix transposes:
\[
(\mathbf{A}^T)^T = \mathbf{A}
\]
\[
(\mathbf{A} + \mathbf{B})^{T} = \mathbf{A}^T + \mathbf{B}^T
\]
\[
(\mathbf{A}\mathbf{B})^{T} = \mathbf{B}^T\mathbf{A}^T
\]

\end{frame}

\subsection{Properties of Different Matrices}

\begin{frame}
\frametitle{Square Matrices}
A matrix that has the same number of rows as columns is called a \textbf{square} matrix.  A $n \times n$ square matrix is said to be of order $n$.  The $n$ elements $a_{11}, a_{22}, \hdots, a_{nn}$ of
\[\mathbf{A} = 
\begin{pmatrix}
    a_{11} & a_{12} & \dots  & a_{1n} \\
    a_{21} & a_{22}  & \dots  & a_{2n} \\
    \vdots & \vdots  & \ddots & \vdots \\
    a_{n1} & a_{n2}  & \dots  & a_{nn}
\end{pmatrix}
\]
 that fall on the imaginary diagonal line extending from the top left corner to the bottom right corner of the matrix are known as \textbf{diagonal elements}.  Any elements of the matrix that are not on the main diagonal are called \textbf{off-diagonal elements}.  Note that the product $\mathbf{AA}$ is only defined if $A$ is square.  If it is, we can write $\mathbf{A}^2 = \mathbf{AA}$ and $\mathbf{A}^k = \mathbf{AAA\cdots A}$.
\end{frame}

\begin{frame}
\frametitle{Symmetric Matrices}
A square matrix $\mathbf{A}$ is symmetric if $\mathbf{A}^T = \mathbf{A}$.  For example, the matrix

\[\begin{pmatrix}
    1 & 1 & 1\\
    1 & 2 & 3 \\
    1 & 3 & 6   
\end{pmatrix}
\]
is symmetric.
\end{frame}

\begin{frame}
\frametitle{Diagonal Matrices}
A square matrix $\mathbf{A}$ is a \textbf{diagonal matrix} if all of its off-diagonal elements are equal to 0.  That is,

\[\mathbf{A} = 
\begin{pmatrix}
    d_{1} & 0 & \dots  & 0 \\
    0 & d_{2}  & \dots  & 0 \\
    \vdots & \vdots  & \ddots & \vdots \\
    0 & 0  & \dots  & d_{n}
\end{pmatrix}
\]

for some $d_1, d_2, \hdots, d_n$.  Diagonal matrices are very easy to work with, since for a $m \times n$ matrix $\mathbf{A}$ and $n \times n$ diagonal matrix $\mathbf{D}$, the $ij$th element of $\mathbf{DA} \text{ (or }\mathbf{AD})$ is equal to $d_i a_{ij}$.  
\end{frame}

\begin{frame}
\frametitle{The Identity Matrix}
The diagonal matrix
\[diag(1,1,\hdots, 1) = 
\begin{pmatrix}
    1 & 0 & \dots  & 0 \\
    0 & 1  & \dots  & 0 \\
    \vdots & \vdots  & \ddots & \vdots \\
    0 & 0  & \dots  & 1
\end{pmatrix}
\]

with diagonal elements all equal to 1 is the identity matrix.  We use the symbol $\mathbf{I}_n$ (or just $\mathbf{I}$ if the context is clear) to denote the identity matrix.  For any matrix $\mathbf{A}$,

\[\mathbf{AI} = \mathbf{IA} = \mathbf{A}\]
\end{frame}

\begin{frame}
\frametitle{Triangular Matrices}
If all the elements of a square matrix that are located below and to the left of the diagonal are 0, then the matrix is called \textbf{upper triangular}.  An upper triangular matrix looks like this:

\[\begin{pmatrix}
    a_{11} & a_{12} & a_{13}& \dots  & a_{1n} \\
    0 & a_{22}  & a_{23} & \dots  & a_{2n} \\
        0 & 0  & a_{33} & \dots  & a_{3n} \\
    \vdots & \vdots & \vdots  & \ddots & \vdots \\
    0 & 0  & 0 & \dots  & a_{nn}
\end{pmatrix}
\]

Similarly, a matrix with all elements above and to the right of the main diagonal equal to zero is a \textbf{lower triangular matrix}.  More formally, an $n \times n$ matrix $\mathbf{A}$ is upper triangular if $a_{ij} = 0$ for $j < i = 1, 2, \hdots, n$.  
\end{frame}

\begin{frame}
\frametitle{Trace of a Square Matrix}
The \textbf{trace} of a square matrix $\mathbf{A}$ of order $n$ is defined to be the sum of the $n$ diagonal elements of $\mathbf{A}$.  This is denoted by the symbol $tr(\mathbf{A}$).  Thus,

\[tr(\mathbf{A}) = a_{11} + a_{22} + \hdots + a_{nn}\]

Some properties of the trace are
\begin{align*}
tr(k\mathbf{A}) &= k\text{ }tr(\mathbf{A}) \\
tr(\mathbf{A} + \mathbf{B}) &= tr(\mathbf{A})+ tr(\mathbf{B}) \\
tr(\mathbf{A}^T) &= tr(\mathbf{A}) \\
tr(\mathbf{AB}) &= tr(\mathbf{BA}) \\
tr(\mathbf{A}_1\mathbf{A}_2\cdots\mathbf{A}_k) &= tr(\mathbf{A}_k\mathbf{A}_1\mathbf{A}_2\cdots \mathbf{A}_{k-1})
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Vectors}
A matrix with only one column, that is, a matrix of the form

\[\begin{pmatrix}
    a_1\\
    a_2\\
    \vdots  \\
    a_m
\end{pmatrix}
\]

is a \textbf{column vector}.  Similarly, a matrix with one row is called a \textbf{row vector}.  We distinguish vectors from matrices by referring to them by a boldface lowercase letter.  So $\mathbf{a}$ is a column vector, and $\mathbf{a}^T$ is a row vector.  The symbol $\mathbf{1}_m$ denotes a column vector with all entries equal to one.\\
\end{frame}

\begin{frame}
\frametitle{Inner Product and Orthogonality}
The \textbf{inner product} of two vectors $\mathbf{a}$ and $\mathbf{b}$ is $\mathbf{a}^T\mathbf{b}$.  The sum of a vector $\mathbf{a}$ can be written as $\mathbf{1}^T\mathbf{a}$.  The mean of a vector is

\[\mathbf{1}^T\mathbf{a}/n = (\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{a}\]

If $\mathbf{a}^T\mathbf{b} = 0$, then $\mathbf{a}$ and $\mathbf{b}$ are \textbf{perpendicular} to one another.  A collection of vectors is \textbf{orthogonal} if and only if they are pairwise perpendicular.
\vspace{5mm}

A vector $\mathbf{a}$ is a \textbf{unit vector} if $\mathbf{a}^T\mathbf{a}$ = 1.
\vspace{5mm}

Null vectors (and null matrices) are denoted by the notation $\mathbf{0}_m$.
\end{frame}


\begin{frame}
\frametitle{Multivariate Normal Distribution}
The \textbf{Mahalanobis distance} from $\mathbf{x}$ to $\mathbf{\mu}$ is given by the quadratic form

$$\Delta = \sqrt{(\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1}(\mathbf{x} - \mathbf{\mu})}$$

An important result is that a random vector $\mathbf{X}$ follows a multivariate distribution if and only if every linear function of $\mathbf{X}$ follows a univariate normal distribution.
\vspace{5mm}

In linear models, we often assume that $\mathbf{\Sigma} = \sigma^2\mathbf{I_d}$, in which case the density function reduces to

$$f(\mathbf{x}|\mu, \sigma) = (2\pi \sigma)^{-d/2} e^{-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T (\mathbf{x} - \mathbf{\mu})}$$
\end{frame}


\begin{frame}
\frametitle{Partitioned Matrices}
A \textbf{submatrix} of a matrix $\mathbf{A}$ can be obtained by striking out rows and/or columns of $\mathbf{A}$.  A \textbf{partitioned matrix} is a $m \times n$ matrix $\mathbf{A}$ that has been expressed in the form 

\[\mathbf{A} = 
\begin{pmatrix}
    \mathbf{A}_{11} & \mathbf{A}_{12} & \dots  & \mathbf{A}_{1c} \\
    \mathbf{A}_{21} & \mathbf{A}_{22}  & \dots  & \mathbf{A}_{2c} \\
    \vdots & \vdots  & \ddots & \vdots \\
    \mathbf{A}_{r1} & \mathbf{A}_{r2}  & \dots  & \mathbf{A}_{rc}
\end{pmatrix}
\]

where the submatrix $\mathbf{A}_{ij}$ is referred to as the $ij$th block of $\mathbf{A}$.  Partitioning matrices can be very useful, particularly when dealing with large sparse matrices (matrices that have many entries equal to zero).
\end{frame}

\begin{frame}
\frametitle{Review Exercises}
\begin{enumerate}
\item Show, using the definition of a matrix product, that $(\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T$.

\item Show that the product of two upper-triangular matrices is also upper triangular.  What is $\mathbf{AB}_{ii}$?

\item Give an example of two symmetric matrices whose product is not symmetric.  When will the product of two symmetric matrices be symmetric? 

\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Problem 1}
Observe that the $ij$th element of ($\mathbf{AB})^T$ equals the $ji$th element of $\mathbf{AB}$, which are both equal to 

$$ \sum_k a_{jk}b_{ki} = \sum_k b_{ki} a_{jk}$$

Since $b_{ki}$ is the $ik$th element of $\mathbf{B}^T$ and $a_{jk}$ is the $kj$th element of $A^T$, the $ij$th element of $\mathbf{B}^T\mathbf{A}^T$ is equal to the $ij$th element of $\mathbf{(AB)}^T$.
\end{frame}

\begin{frame}
\frametitle{Problem 2}
By definition, the $ij$th element of $\mathbf{AB}$ is $\sum_{k=1}^n a_{ik} b_{kj}$.  Suppose the $\mathbf{A}$ and $\mathbf{B}$ are both upper triangular.  Then $a_{ik} = 0$ for $k < i$ and $b_{kj} = 0$ for $k > j$.  Therefore,

$$\sum_{k = 1}^n a_{ik}b_{kj} = \sum_{k = 1}^{i-1} (0)b_{kj} + a_{ii}b_{ij} + \sum_{k = i+1}^n a_{ik}(0) = a_{ii}b_{ij}$$

Since $\mathbf{B}$ is upper triangular, $b_{ij} = 0$ for $j < i$ so $\mathbf{AB}$ is upper triangular.  The diagonal elements of $\mathbf{AB}$ are $a_{ii}b_{ii}$.
\end{frame}

\begin{frame}
\frametitle{Problem 3}
Answers may vary.  Let $$\mathbf{A} = \begin{pmatrix} 1 & 2 \\ 2 & 3\end{pmatrix} \qquad \mathbf{B} = \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix}$$

Then
$$\mathbf{AB} = \begin{pmatrix} 5 & 8 \\ 4 & 7\end{pmatrix} $$

which is not symmetric.  In general, the product of two symmetric matrices will be symmetric if and only if the two matrices commute.  To see this, note that if $\mathbf{AB}$ is symmetric,

$$\mathbf{AB} = \mathbf{(AB)}^T = \mathbf{B}^T\mathbf{A}^T = \mathbf{BA}$$

If $\mathbf{B}$ and $\mathbf{A}$ commute, then

$$\mathbf{AB} = \mathbf{BA} = \mathbf{B}^T\mathbf{A}^T = \mathbf{(AB)}^T $$
\end{frame}

\section{Matrices: Beyond Basic Operations}

\begin{frame}
\frametitle{Linear Independence}
A nonempty finite set $\{\mathbf{A}_1, \mathbf{A}_2, \hdots, \mathbf{A_k}\}$ of $m \times n$ matrices is \textbf{linearly dependent} if there exist scalars $x_1, x_2, \hdots, x_k$, not all equal to zero, such that

\[x_1\mathbf{A}_1 + x_2\mathbf{A_2} + \cdots + x_k\mathbf{A}_k = \mathbf{0}\]

If no such scalars exist, the set is said to be \textbf{linearly independent}. 
\vspace{5mm}

In addition, a set of two or more $m \times n$ matrices is linearly dependent if and only if at least one of the matrices can be expressed as a linear combination of the other matrices ($\mathbf{A_j}$ is expressible as a linear combination of $\mathbf{A_1}, \hdots, \mathbf{A}_{j-1}, \mathbf{A}_{j+1}, \hdots \mathbf{A_k}$).
\end{frame}

\begin{frame}
\frametitle{Row and Column Space}
The \textbf{column space} of a $m \times n$ $\mathbf{A}$ is the set whose elements consist of all $m$ dimensional vectors that can be expressed as linear combinations of the $n$ columns of $\mathbf{A}$.  The elements of the column space of $\mathbf{A}$ are all $m$ dimensional vectors of the form

\[x_1 \mathbf{a}_1 + x_2\mathbf{a}_2 + \cdots + 
x_n\mathbf{a}_n\]

where $x_1, x_2, \hdots, x_n$ are scalars and $\mathbf{a}_1, \hdots, \mathbf{a}_n$ are the columns of $\mathbf{A}$.  The row space is defined similarly, by finding all $n$ dimensional vectors that can be expressed as a linear combination of the rows of $\mathbf{A}$. 
\end{frame}

\begin{frame}
\frametitle{Column Space: Example}
 For example, the column space of the $3 \times 4$ matrix

\[\begin{pmatrix}
\phantom{-}2 & -4 & 0 & 0\\
-1 & \phantom{-}2 & 0 & 0\\
\phantom{-}0 & \phantom{-}0 & 1 & 2
\end{pmatrix}\]

includes the column vector

\[
\begin{pmatrix}
\phantom{-}4\\
-2\\
-3
\end{pmatrix}
=
2\begin{pmatrix}
\phantom{-}2\\
-1\\
\phantom{-}0
\end{pmatrix}
+
0\begin{pmatrix}
-4\\
\phantom{-}2\\
\phantom{-}0
\end{pmatrix}
-
3\begin{pmatrix}
0\\
0\\
1
\end{pmatrix}
+
0\begin{pmatrix}
0\\
0\\
2
\end{pmatrix}
\]

but not the column vector $(2,0,0)^T$.  
\end{frame}

\begin{frame}
\frametitle{Linear Spaces}
Row spaces and column spaces of a matrix are examples of a more general concept called a \textbf{linear space}.  A nonempty set $\mathcal{V}$ is a linear space if it is closed under element-wise multiplication and scalar multiplication (i.e. $\mathbf{A}$, $\mathbf{B} \in \mathcal{V} \Rightarrow \mathbf{A} + \mathbf{B} \in \mathcal{V}, k\mathbf{A} \in \mathcal{V}$).
\vspace{0.5cm}

A subset $\mathcal{U}$ of a linear space $\mathcal{V}$ is called a \textbf{subspace} of $\mathcal{V}$ if $\mathcal{U}$ is a linear space.  For example, the column space $\mathcal{C}(\mathbf{A})$ of a $m \times n$ matrix $\mathbf{A}$ is a subspace of $\mathbb{R}^m$ and the row space $\mathcal{C}(\mathbf{A})$ of $\mathbf{A}$ is a subspace of $
\mathbb{R}^n$.
\end{frame}

\begin{frame}
\frametitle{Basis}
The \textbf{span} of a finite, nonempty set of matrices $S = \{\mathbf{A}_1, \mathbf{A}_2, \hdots, \mathbf{A_k}\}$ is the set of all matrices that are expressible as a linear combination of the elements of $S$.  The span is denoted by sp($S$) and is a linear space.
\vspace{5mm}

A \textbf{basis} for a linear space $\mathcal{V}$ is a finite set of linearly independent matrices in $\mathcal{V}$ that spans $\mathcal{V}$.  Note that if the columns of $\mathbf{A}$ are linearly independent, then the columns of $\mathbf{A}$ are a basis for $\mathcal{C}(\mathbf{A})$.
\vspace{5mm}

For example, $\{(1,0,0)^T, (0,1,0)^T, (0,0,1)^T\}$ and $\{(1,1,0)^T, (1,2,0)^T, (0,2,3)^T\}$ are both bases for $\mathbb{R}^3$.  However, $\{(1,0,0)^T, (0,1,0)^T, (0,0,1)^T, (0,2,3)^T\}$ is not a basis for $\mathbb{R}^3$ since the set is linearly dependent.
\end{frame}

\begin{frame}
\frametitle{Rank}
The number of matrices in a basis for a linear space $\mathcal{V}$ is called the \textbf{dimension} of $\mathcal{V}$ and is denoted by dim($\mathcal{V}$).  The \textbf{row rank} of a matrix $\mathbf{A}$ is the dimension of the row space of $\mathbf{A}$.  Similarly, the \textbf{column rank} of a matrix $\mathbf{A}$ is the dimension of the column space of $\mathbf{A}$. 

\vspace{5mm}

An important result in matrix algebra is the row rank and the column of a matrix are equal.  This common value is called the \textbf{rank} of $\mathbf{A}$ and denoted by rank($\mathbf{A}$).

\vspace{5mm}

A matrix is said to have \textbf{full row rank} if rank($\mathbf{A}) = m$ and \textbf{full column rank} if rank($\mathbf{A}) = n$.  Square matrices ($m = n$) are said to be \textbf{full rank} or \textbf{nonsingular} if it has both full row rank and full column rank.  A $n \times n$ matrix with rank less than $n$ is \textbf{singular}.  
\end{frame}

\begin{frame}
\frametitle{Rank: Example}
\[\mathbf{A} = \begin{pmatrix}
    1 & 3 & 4\\
    2 & 6 & 8 \\
    5 & 7 & 12   
\end{pmatrix}
\]

\vspace{5mm}

is a singular matrix, since rank($\mathbf{A}) = 2$, but $\mathbf{A}$ is a $ 3 \times 3$ matrix.
\end{frame}

\begin{frame}
\frametitle{Matrix Inverses}
A $n \times n$ matrix $\mathbf{A}$ is invertible if and only if $\mathbf{A}$ is nonsingular.  If $\mathbf{A}$ is invertible, then there exists a unique inverse matrix $\mathbf{A}^{-1}$ such that $\mathbf{AA^{-1}} = \mathbf{A^{-1}A} = \mathbf{I}$.  
\vspace{5mm}

For any $n \times n$ nonsingular diagonal matrix $\mathbf{D}$,

\[ \mathbf{D}^{-1} = diag(1/d_1, 1/d_2, \hdots, 1/d_n)\]

The inverse of a $2 \times 2$ nonsingular matrix

\begin{align*}
\mathbf{A} &= \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}  \\  
\end{pmatrix}\\
    \mathbf{A}^{-1} &= (1/k)\begin{pmatrix}
    \phantom{-}a_{22} & -a_{12} \\
    -a_{21} & \phantom{-}a_{11}  \\  
\end{pmatrix}\\
\end{align*}

where $k = a_{22}a_{11} - a_{12}a_{21}$.
\end{frame}

 \begin{frame}
\frametitle{Intuition: Matrix Inverses}

\href{https://www.youtube.com/watch?v=uQhTuRlWMxw&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=7}{\textbf{Inverse matrices}, by \textit{3Blue1Brown}}
\end{frame}

\begin{frame}
\frametitle{Inverse Properties}
Let $\mathbf{A}$ be a nonsingular $n \times n$ matrix.  Then

\[(\mathbf{A}^{-1})^{-1} = \mathbf{A}\]

\[(k\mathbf{A})^{-1} = 1/k\mathbf{A}^{-1}\]

for any non-zero scalar $k$.  If $\mathbf{B}$ is also a nonsingular $n \times n$ matrix, then 

\[((\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}\]

and more generally,

\[(\mathbf{A_1 A_2 \cdots A_k})^{-1} = \mathbf{A_k}^{-1}\cdots \mathbf{A_2}^{-1} \mathbf{A}_1^{-1}\]
\end{frame}



\begin{frame}
\frametitle{Orthogonal Matrices}
A nonsingular matrix $\mathbf{A}$ is an \textbf{orthogonal matrix} if $\mathbf{A}^T = \mathbf{A}^{-1}$.  Equivalently, $\mathbf{A}^T\mathbf{A} = \mathbf{AA}^T = \mathbf{I}$   

\vspace{5mm}

Examples of orthogonal matrices include the identity matrix $\mathbf{I}_n$ and, for any angle $\theta$, the $2 \times 2$ matrix

\[
 \begin{pmatrix}
    \phantom{-}\cos (\theta) & \sin (\theta) \\
    -\sin (\theta) & \cos (\theta)  \\  
\end{pmatrix}
\]

\textbf{ADD EXERCISE TO SHOW THAT THIS MATRIX IS ORTHOGONAL?}

\end{frame}

\begin{frame}
\frametitle{Generalized Inverses}
A \textbf{generalized inverse} or \textbf{pseudoinverse} of an $m \times n$ matrix $\mathbf{A}$ is any $n \times m$ matrix $\mathbf{G}$ such that $$\mathbf{AGA} = \mathbf{A}$$.  

If $\mathbf{A}$ is nonsingular, then it has a unique generalized inverse.  Otherwise, $\mathbf{A}$ has an infinite number of generalized inverses.  There are many properties of generalized inverses that are useful when solving a linear system, but they are not presented here for brevity. 

\textbf{I THINK WE CAN CUT THIS. JUST MAKE A NOT EARLIER THAT PSEUDOINVERSES EXIST}

\end{frame}

\begin{frame}
\frametitle{Determinant}
The \textbf{determinant} of a square $n \times n$ matrix $\mathbf{A}$ is denoted by either $|\mathbf{A}|$ or det($\mathbf{A}$).  The minor $\mathbf{M}_{ij}$ of element $A_{ij}$ is the $n - 1 \times n-1$ matrix that is formed by removing the $i$th row and $j$th column from $\mathbf{A}$.  The cofactor of $A_{ij}$ is $C_{ij} = (-1)^{i + j}|\mathbf{M}_{ij}$.  Expanding along the $i$th row, 

$$|\mathbf{A}| = \sum_{j = 1}^n A_{ij}C_{ij}$$

Note that $|\mathbf{A}^T| = |\mathbf{A}|$ and that $|k\mathbf{A}| = k^n|\mathbf{A}|$.  $\mathbf{A}$ is singular if $|\mathbf{A}| = 0$ and nonsingular otherwise.
\end{frame}

\begin{frame}
\frametitle{Projection Matrices}
A square matrix $\mathbf{A}$ is \textbf{idempotent} if $\mathbf{A}^2 = \mathbf{A}$.  A square matrix $\mathbf{P}$ is a \textbf{projection matrix} if and only if $\mathbf{P}$ is idempotent.  If $\mathbf{P}$ is also orthogonal, then $\mathbf{P}$ is called an orthogonal projector.

\textbf{ADD SOME EXAMPLES PLEASE}

\vspace{5mm}

A very useful property of projection matrices is that their rank is equal to their trace, namely

\[tr(\mathbf{A}) = rank(\mathbf{A})\]

The identity matrix $\mathbf{I}_n$ is the only full rank idempotent matrix.  
\end{frame}

\begin{frame}
\frametitle{Application to Least Squares}
A primary application of projection matrices is linear models and the method of least squares.  One important result is that the projection $\mathbf{z}$ of a $n$-dimensional vector $\mathbf{y}$ onto the column space of a $n \times p$ matrix $\mathbf{X}$ is

\[\mathbf{z} = \mathbf{X(X^TX)}^{-1}\mathbf{X^Ty}\]

Letting $\mathbf{\hat{\beta}}$ be the ordinary least squares estimate of $\mathbf{\beta}$, where $\mathbf{y} = \mathbf{X\beta} + \mathbf{\epsilon}$, we have that the fitted values $\mathbf{X}\hat{\mathbf{\beta}}$ is the projection of the response vector $\mathbf{y}$ onto the column space of the covariate matrix $\mathbf{X}$.

\textbf{TURN THIS INTO AN EXERCISE}

\end{frame}

\begin{frame}
\frametitle{Quadratic Forms}
If $\mathbf{A}$ is an $n \times n$ matrix and $\mathbf{x}$ is an $n$-dimensional vector, then a \textbf{quadratic form} is 

\[\mathbf{x}^T\mathbf{Ax}\]

$\mathbf{A}$ is \textbf{positive definite} if, for all $\mathbf{x} \neq \mathbf{0}$, $\mathbf{x}^T\mathbf{Ax} > 0$.  If instead $\mathbf{x}^T\mathbf{Ax} \geq 0$, then $\mathbf{A}$ is \textbf{nonnegative definite}.
\vspace{5mm}

Symmetric nonnegative definite matrices are encountered frequently in linear models and other areas of statistics.  In particular, the covariance matrix (we'll get to that next section) is nonnegative definite.  In addition, any positive definite matrix is nonsingular.

\end{frame}

\begin{frame}
\frametitle{Eigenvalues and Eigenvectors}
A scalar $\lambda$ is said to be an \textbf{eigenvalue} of an $n \times n$ matrix $\mathbf{A}$ if there exists a non-null vector $\mathbf{x}$ such that

\[\mathbf{Ax} = \lambda\mathbf{x}\]

The set of eigenvalues is called the \textbf{spectrum} of $\mathbf{A}$.  All eigenvalues of positive-definite matrices are positive and all eigenvalues of nonnegative-definite matrices are nonnegative. 
\vspace{5mm}

 A non-null vector $\mathbf{x}$ is an \textbf{eigenvector} of $\mathbf{A}$ if there exists a scalar $\lambda$ such that $\mathbf{Ax} = \lambda\mathbf{x}$.
\end{frame}

 \begin{frame}
\frametitle{Intuition: Eigenvectors and Eigenvalues}

\href{https://www.youtube.com/watch?v=PFDu9oVAE-g&index=13&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}{\textbf{Eigenvectors and eigenvalues}, by \textit{3Blue1Brown}}
\end{frame}

\begin{frame}
\frametitle{Eigendecomposition}
If  $\mathbf{A}$ is a symmetric $n \times n$ matrix, eigenvectors $\mathbf{v}_j$ and $\mathbf{v}_k$ associated with distinct eigenvalues $\lambda_j \neq \lambda_k$ are orthogonal.  In addition, if $\mathbf{V} = (\mathbf{v}_1, \mathbf{v}_2, \hdots, \mathbf{v_n})$, then the spectral theorem expresses $\mathbf{A}$ as a weighted average of rank 1 matrices,

\[\mathbf{A} = \mathbf{V\Lambda V}^T = \sum_{j=1}^n \lambda_j \mathbf{v}_j \mathbf{v}_j ^T\]

This decomposition of a square matrix into its eigenvalues and eigenvectors is called the \textbf{eigendecomposition}.  This decomposition will always exist if the matrix is symmetric.  The rank of $\mathbf{A}$ is the number of nonzero eigenvalues, and

\[tr(\mathbf{A}) = \sum_{j = 1}^n \lambda_j \qquad |\mathbf{A}| = \prod_{j=1}^n \lambda_j \]

\end{frame}

\begin{frame}
\frametitle{Matrix Decompositions}
The Cholesky decomposition takes a positive definite matrix $\mathbf{A}$ and decomposes it into $\mathbf{U}^T\mathbf{U}$, where $\mathbf{U}$ is an upper triangular matrix.  Cholesky factorization is substantially faster than the more general LU factorization because pivoting is not required.  

\vspace{5mm}

The QR decomposition decomposes a general $m \times n$ matrix $\mathbf{A}$ into $\mathbf{QR}$, where $\mathbf{Q}$ is an orthogonal $m \times m$ matrix and $\mathbf{R}$ is an upper triangular $m \times n$ matrix.  This decomposition is useful for solving least squares problems and numerically approximating eigenvectors. 
\end{frame}

\begin{frame}
\frametitle{Singular Value Decomposition}
A $m \times n$ matrix $\mathbf{A}$ can be expressed as
$$\mathbf{A} = \mathbf{UDV}^T$$ where $\mathbf{U}$ is an $m \times n$ matrix such that $\mathbf{U}^T\mathbf{U} = \mathbf{I_m}$, $\mathbf{V}^T\mathbf{V} = \mathbf{I_n}$, and $\mathbf{D}$ is a nonnegative diagonal matrix with entries $d_1 \geq \cdots \geq d_n$.
\vspace{5mm}

This representation is called the \textbf{singular value decomposition} of $\mathbf{A}$ (or SVD).  The SVD provides a description of the within-row variation via $\mathbf{V}$ and a description of the within column variation via $\mathbf{U}$.  This is very useful when describing large data matrices with a low-dimensional approximation.

\vspace{5mm}

The SVD of a symmetric matrix is identical to its eigendecomposition.
\end{frame}

\begin{frame}
\frametitle{The Jacobian Matrix}
Let $\mathbf{x} = (x_1, \hdots, x_K)^T$ be a $k$ dimensional vector and let 

\[\mathbf{y} = (y_1, \hdots, y_J)^T = (f_1(\mathbf{x}), \hdots, f_J(\mathbf{x}))^T = \mathbf{f}(\mathbf{x})\]

be a $J$ dimensional vector, where $\mathbf{f}: \mathbb{R}^K \rightarrow \mathbb{R}^J$.  Then the partial derivative of $\mathbf{y}$ with respect to $\mathbf{x}^T$ yields the $J \times K$ \textbf{Jacobian matrix}

\[\mathbf{J_xy} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}^T} 
=
\begin{pmatrix}
    \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \dots  & \frac{\partial y_1}{\partial x_K} \\
    \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \dots  & \frac{\partial y_2}{\partial x_K}\\
    \vdots & \vdots  & \ddots & \vdots \\
    \frac{\partial y_J}{\partial x_1} & \frac{\partial y_J}{\partial x_2} & \dots  & \frac{\partial y_J}{\partial x_K}
\end{pmatrix}
\]

The \textbf{Jacobian} of the tranformation $\mathbf{y} = \mathbf{f(x)}$ is 
$$J = |\mathbf{J_xy}|$$
\end{frame}

\begin{frame}
\frametitle{Gradient and Hessian}
If $y = f(\mathbf{x})$ is a scalar, then the \textbf{gradient} vector is

$$\Delta_xy = \frac{\partial y}{\partial \mathbf{x}} = \left(\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \hdots, \frac{\partial y}{\partial x_K} \right)^T = \left(\frac{\partial y}{\partial \mathbf{x}^T}\right)^T = (\mathbf{J}_xy)^T$$

The $K \times K$ matrix of second order partial derivatives is called the \textbf{Hessian matrix}.  This is defined mathematically as

$$\mathbf{H}_{\mathbf{x}}y = \frac{\partial }{\partial \mathbf{x}} \left(\frac{\partial y}{\partial \mathbf{x}}\right)^T = \frac{\partial ^2y}{\partial \mathbf{x} \partial \mathbf{x}^T} = 
\begin{pmatrix}
    \frac{\partial ^2 y}{\partial x^2_1} & \frac{\partial ^2y}{\partial x_1 x_2} & \dots  & \frac{\partial ^2y}{\partial x_1 x_K} \\
    \frac{\partial ^2 y}{\partial x_1x_2} & \frac{\partial ^2y}{\partial x^2_2} & \dots  & \frac{\partial ^2y}{\partial x_2 x_K}\\
    \vdots & \vdots  & \ddots & \vdots \\
    \frac{\partial ^2y}{\partial x_1 x_K} & \frac{\partial ^2y}{\partial x_2 x_K} & \dots  & \frac{\partial ^2y}{\partial x^2_K}
\end{pmatrix}
$$

The gradient and Hessian, if they can be found in closed form, are very useful when optimizing a scalar function $y = f(\mathbf{x})$. 
\end{frame}

\begin{frame}
\frametitle{Matrix Differentiation}
The derivative of a $J \times K$ matrix $\mathbf{A}$ with respect to an r dimensional vector $x$ is the $(Jr \times K)$ matrix of derivatives of $\mathbf{A}$ with respect to each element of $\mathbf{x}$.  In other words,

\[\frac{\partial \mathbf{A}}{\partial \mathbf{x}} = \left(\frac{\partial \mathbf{A}^T}{\partial x_1}, \hdots, \frac{\partial \mathbf{A}^T}{\partial x_r}  \right)^T\]

Similar to normal differentiation, we have the following rules:

\begin{align*}
\frac{\partial (\alpha \mathbf{A})}{\partial \mathbf{x}} &= \alpha \frac{\partial \mathbf{A}}{\partial \mathbf{x}}\\
\frac{\partial (\mathbf{A+B})}{\partial \mathbf{x}} &=  \frac{\partial \mathbf{A}}{\partial \mathbf{x}} + \frac{\partial \mathbf{B}}{\partial \mathbf{x}}\\
\frac{\partial (\mathbf{AB})}{\partial \mathbf{x}} &=  \left(\frac{\partial \mathbf{A}}{\partial \mathbf{x}}\right)\mathbf{B} + \mathbf{A}\left(\frac{\partial \mathbf{B}}{\partial \mathbf{x}}\right)\\
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Review Exercises}
\begin{enumerate}
\item Show that the product of two orthogonal matrices is also orthogonal, assuming that the product is nonsingular.

\item Show that if $\mathbf{P}$ is a projection matrix, then $\mathbf{I} - \mathbf{P}$ is also a projection matrix.

\item Show that the eigenvalues of an idempotent matrix are either 0 or 1.

\item What is the Jacobian matrix for the polar coordinate transformation? ($x = r \cos (\theta)$, $y = r \sin (\theta))$
\end{enumerate}
\end{frame}


\section{Random Matrices and Multivariate Statistics}
\begin{frame}
\frametitle{Random Vectors}
If we have $d$ random variables $X_1, X_2, \hdots, X_d$, each defined on the real line, we can write them as the $d$ dimensional column vector $$\mathbf{X} = (X_1, \cdots X_d)^T$$

which we call a $d$-dimensional \textbf{random vector}.  The joint distribution function of the random vector $\mathbf{X}$ is

\begin{align*}
F_X(\mathbf{x}) &= F_X(x_1, \hdots, x_d) \\
&= P(X_1 \leq x_1, \hdots, X_d \leq x_d)\\
&= P(\mathbf{X} \leq \mathbf{x})
\end{align*}

If $F_X$ is absolutely continuous, then the joint density function $f_X$ of $\mathbf{X}$ is

$$f_X(\mathbf{x}) = f_X(x_1, \hdots, x_d) = \frac{\partial ^d F_X(x_1, \hdots, x_d)}{\partial x_1 \cdots \partial x_d}$$
\end{frame}

\begin{frame}
\frametitle{Random Vectors}
To find the marginal density of a subset of the $d$ variables, you can just integrate the others out.  For example, if we have a joint bivariate density $f_{X_1,X_2}(x_1, x_2)$, then 

$$f_{X_1}(x_1) = \int_{-\infty}^{\infty} f_{X_1,X_2}(x_1, x_2) dx_2 \qquad f_{X_2}(x_2) = \int_{-\infty}^{\infty} f_{X_1,X_2}(x_1, x_2) dx_1$$

The components of a random vector $\mathbf{X}$ are \textbf{independent} if the joint distribution function is a product of the marginal distribution functions

$$F_X(\mathbf{x}) = \prod_{i=1}^d F_i(x_i)$$

In addition, the joint density is the product of marginals

$$f_X(\mathbf{x}) = \prod_{i=1}^d f_i(x_i)$$
\end{frame}

\begin{frame}
\frametitle{Expectation and Covariance}
If $\mathbf{X}$ is a random vector with values in $\mathbb{R}^d$, then its expected value is given by the $d$ dimensional vector

$$\mathbf{\mu}_X = E(\mathbf{X}) = (E(X_1), \cdots, E(X_d)) = (\mu_1, \cdots, \mu_d)^T$$

and the $d \times d$ \textbf{covariance matrix} of $\mathbf{X}$ is 

\begin{align*}
\mathbf{\Sigma}_{XX} &= \text{cov}(\mathbf{X}, \mathbf{X}) \\
&= E[(\mathbf{X} - \mathbf{\mu}_X)(\mathbf{X} - \mathbf{\mu}_X)^T]\\
&= E[(X_1 - \mu_1, \cdots, X_d - \mu_d) (X_1 - \mu_1, \cdots, X_d - \mu_d)^T]\\
&= 
\begin{pmatrix}
    \sigma_1^2 & \sigma_{12} & \dots  & \sigma_{1d}\\
    \sigma_{21} & \sigma_2^2  & \dots  & \sigma_{2d}\\
    \vdots & \vdots  & \ddots & \vdots \\
    \sigma_{d1} & \sigma_{d2}  & \dots  & \sigma_d^2
\end{pmatrix}
\end{align*} 
\end{frame}

\begin{frame}
\frametitle{Correlation Matrix}
The \textbf{correlation matrix} of $\mathbf{X}$ can be obtained by from $\mathbf{\Sigma}_{XX}$ by dividing the $i$th row by $\sigma_i$ and the $j$th column by $\sigma_j$.  The $d \times d$ matrix is then

$$P_{XX} = \begin{pmatrix}
    1 & \rho_{12} & \dots  & \rho_{1d}\\
    \rho_{21} & 1  & \dots  & \rho_{2d}\\
    \vdots & \vdots  & \ddots & \vdots \\
    \rho_{d1} & \rho_{d2}  & \dots  & 1
\end{pmatrix}$$

where 

$$\rho_{ij} = \rho_{ji} = \begin{cases}
\frac{\sigma_{ij}}{\sigma_i \sigma_j} & i \neq j\\
1 & \text{otherwise}
\end{cases}
$$

is the pairwise correlation coefficient between $X_i$ and $X_j$.  The correlation coefficient will always lie between $-1$ and $1$ and is a measure of association between $X_i$ and $X_j$.
\end{frame}

\begin{frame}
\frametitle{Linear Functions of Random Vectors}
If $\mathbf{Y}$ is a linear function of $\mathbf{X}$ such that

$$\mathbf{Y} = \mathbf{AX} + \mathbf{b}$$

the mean vector and covariance matrix of $\mathbf{Y}$ is given by

\begin{align*}
\mathbf{\mu}_Y &= \mathbf{A\mu}_x + \mathbf{b}\\
\mathbf{\Sigma}_{YY} &= \mathbf{A\Sigma_{XX}A}^T
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Multivariate Normal Distribution}
The form of the multivariate normal looks similar to that of the univariate normal.  A random $d$ vector $\mathbf{X}$ follows a multivariate normal distribution with mean vector $\mu$ and positive definite symmetric covariance matrix $\mathbf{\Sigma}$ if it has the density function

$$f(\mathbf{x}|\mu, \mathbf{\Sigma}) = (2\pi)^{-d/2}|\mathbf{\Sigma}|^{-1/2} e^{-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1}(\mathbf{x} - \mathbf{\mu})}$$

We notationally denote a $d$ dimensional normal distribution as 

$$\mathbf{X} \sim N_d(\mu, \mathbf{\Sigma})$$
\end{frame}

\begin{frame}
\frametitle{Partitioned Random Vectors}
Suppose we have two random vectors $\mathbf{X}$ and $\mathbf{Y}$, where $\mathbf{X}$ has $d_1$ components and $\mathbf{Y}$ had $d_2$ components.  Let $\mathbf{Z}$ be the random $d_1 + d_2$ vector

$$\mathbf{Z} = \begin{pmatrix} \mathbf{X}  \\ \mathbf{Y} \end{pmatrix}$$

Then the expected value and covariance matrix of $\mathbf{Z}$ is given by 
\begin{align*}
\mathbf{\mu}_Z &= E[\mathbf{Z}] = \begin{pmatrix} E[\mathbf{X}] \\ E[\mathbf{Y}] \end{pmatrix} = \begin{pmatrix} \mathbf{\mu}_X \\ \mathbf{\mu}_Y \end{pmatrix}\\
\mathbf{\Sigma}_{ZZ} &= \begin{pmatrix} cov(\mathbf{X}, \mathbf{X}) & cov(\mathbf{X}, \mathbf{Y}) \\ cov(\mathbf{Y}, \mathbf{X}) & cov(\mathbf{Y}, \mathbf{Y})  \end{pmatrix}\\
&= \begin{pmatrix}
\mathbf{\Sigma_{XX}} & \mathbf{\Sigma}_{XY}\\
\mathbf{\Sigma}_{YX} & \mathbf{\Sigma}_{YY}
\end{pmatrix}
\end{align*}

where $\mathbf{\Sigma}_{XY} = \mathbf{\Sigma}_{YX}^T$. 
\end{frame}

\begin{frame}
\frametitle{Marginal/Conditional Normal Distribution}
The marginal distribution of $\mathbf{Y}$ is

$$\mathbf{Y} \sim N_{d_2}(\mu_y, \mathbf{\Sigma}_{YY}$$

The conditional distribution of $\mathbf{Y}$ given that $\mathbf{X} = \mathbf{x}$ is multivariate normal with mean vector and covariance matrix given by

\begin{align*}
\mu_{Y|X} &= \mu_Y + \mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} (\mathbf{x} - \mu_X)\\
\mathbf{\Sigma}_{Y|X} &= \mathbf{\Sigma}_{YY} - \mathbf{\Sigma}_{YX}\mathbf{\Sigma}_{XX}^{-1}\mathbf{\Sigma}_{XY}
\end{align*}

\end{frame}


\begin{frame}
\frametitle{Problem 1}
Let $\mathbf{P}$ and $\mathbf{Q}$ both be $n \times n$ orthogonal matrices.  Then

 $$(\mathbf{PQ})^{-1} = \mathbf{Q}^{-1}\mathbf{P}^{-1} = \mathbf{Q}^T \mathbf{P}^T = (\mathbf{PQ})^T$$
\end{frame}

\begin{frame}
\frametitle{Problem 2}
If a matrix is idempotent, then it is a projection matrix.

$$(\mathbf{I} - \mathbf{P})(\mathbf{I} - \mathbf{P}) = \mathbf{I}^2 - 2\mathbf{IP} + \mathbf{P}^2 = \mathbf{I} - 2\mathbf{P} + \mathbf{P} = \mathbf{I} - \mathbf{P}$$

Hence, $\mathbf{I} - \mathbf{P}$ is idempotent and it is therefore a projection matrix.
\end{frame}

\begin{frame}
\frametitle{Problem 3}
Let $\lambda$ be an eigenvalue of an idempotent matrix $\mathbf{A}$.  Then there exists an eigenvector $\mathbf{x}$ such that $\mathbf{Ax} = \lambda \mathbf{x}$.  In addition, since $\mathbf{A} = \mathbf{A}^2$, $\lambda^2 \mathbf{x} = \lambda \mathbf{x}$.  Therefore, since $\lambda = \lambda^2$, $\lambda$ must be either 0 or 1.
\end{frame}

\begin{frame}
\frametitle{Problem 4}
The Jacobian matrix in this case is

$$\begin{pmatrix} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}\end{pmatrix} = \begin{pmatrix} \cos \theta & -r \sin \theta \\ \sin \theta & r \cos \theta \end{pmatrix}$$
\end{frame}




\begin{frame}{Reference Guide}
\begin{itemize}
\item{\emph{Matrix Algebra from a Statistician's Perspective} - Harville}
\item{\emph{Matrix Algebra: Theory, Computations, and Applications in Statistics} - Gentle}
\item{\emph{The Matrix Cookbook} - Petersen and Pedersen}
\item{\emph{Applied Multivariate Statistical Analysis} - Johnson and Wichern}
\item{\emph{Modern Multivariate Statistical Techniques} - Izenman}
\end{itemize}
\end{frame}

\end{document}
